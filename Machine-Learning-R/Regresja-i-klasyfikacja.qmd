---
title: "Regresja liniowa, regresja logistyczna, algorytm k najbliższych sąsiadów"
author: "Paweł Wieczyński"
format: html
editor: visual
---

```{r}
pacman::p_load(tidyverse, broom, car, GGally, caret, ISLR, pROC, class)
options(scipen = 20)
theme_set(theme_bw())
```

## Regresja liniowa

### Przykład z jedną zmienną objaśniającą

#### Krok 1: Przygotowanie danych

-   wczytujemy dane

-   sprawdzamy strukturę danych, podstawowe statystyki opisowe

-   sprawdzamy dane wizualnie za pomocą wykresów

-   modyfikacja zmiennych, dodanie nowych zmiennych

-   podział danych na zbiór uczący, zbiór testowy, zbiór walidacyjny (pominiemy w tym przykładzie).

```{r}
cars = datasets::cars
str(cars)
summary(cars)
```

```{r}
cars$speed = cars$speed * 1.609344
cars$dist = cars$dist * 0.3048
summary(cars)
```

```{r}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point()

cars %>%
  pivot_longer(
    cols = 1:2
    , names_to = 'Variable'
    , values_to = 'Value') %>%
  ggplot(aes(x = Variable, y = Value, fill = Variable)) +
  geom_boxplot() +
  theme(legend.position = 'none')
```

#### Krok 2: Trenowanie modelu

Równanie regresji liniowej ma postać:

$$ y_i = \alpha + \beta \ x_i + \epsilon_i $$

dla $i = 1, 2, \dots, n$ gdzie:

-   $y_i$ nazywamy zmienną zależną / zmienną objaśnianą / zmienną celu

-   $x_i$ nazywamy zmienną niezależną / zmienną objaśniającą / regresorem

-   $\alpha, \beta$ to parametry regresji, które chcemy oszacować

-   $\epsilon_i$ to składnik losowy / reszty modelu, czyli różnice między wartościami obserwacji $y_i$ a wartościami prognozowanymi przez model $\widehat{y_i}$.

Spośród wszystkich możliwych kombinacji $\alpha, \beta$ wybieramy takie $\widehat{\alpha}, \widehat{\beta}$, które minimalizują sumę kwadratów reszt, tzn.

$$ \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left( y_i - \widehat{y} \right)^2 \ \longrightarrow \ \min $$ Jest to tzw. **metoda najmniejszych kwadratów** (ang. metoda OLS).

```{r}
model_1 = lm(dist ~ speed, data = cars)
model_2 = lm(dist ~ I(speed^2), data = cars)
```

#### Krok 3: Ocena modelu

Za pomocą funkcji *summary()* wyświetlamy podsumowanie modelu. Elementy, na które warto zwrócić uwagę, to:

-   *Estimate* - oszacowania współczynników regresji $\widehat{\alpha}, \widehat{\beta}$

-   *Pr(\>\|t\|)* - wartości *p-value* oznaczające czy dane współczynniki regresji są istotne statystycznie; na ogół przyjmujemy, że współczynniki są istotne, jeżeli *p-value* jest mniejsze od $0.05$

-   *F-statistic* - test sprawdzający, czy model jako całość jest istotny statystycznie; na ogół przyjmujemy, że model jest istotny, jeśli *p-value* odpowiadająca temu testowi jest mniejsza od $0.05$

-   *Multiple R-squared* - współczynnik determinancji $R^2$ oznacza jaki odsetek zmienności zmiennej zależnej został wyjaśniony przez ten model. Wartość $R^2$ przyjmuje wartości od $0$ do $1$, przy czym im większa wartość, tym lepiej dopasowany model.

-   *Adjusted R-squared* - skorygowany $R^2$ nakłada dodatkową penalizację na ilość zmiennych w modelu.

```{r}
summary(model_1)
summary(model_2)
```

```{r}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(formula = 'y ~ x', method = 'lm', se = FALSE) +
  geom_smooth(formula = 'y ~ I(x^2)', method = 'lm', se = FALSE, color = 'red')
```

#### Krok 4: Prognozowanie

```{r}
new_data = data.frame(speed = c(30, 50, 80))
predict(model_1, newdata = new_data)
predict(model_2, newdata = new_data)
```

Do oceny jakości prognoz mamy wiele różnych miar. Najpopularniejsze z nich to:

-   Współczynnik $R^2$, który możemy policzyć również jako kwadrat korelacji między $y$ oraz $\widehat{y}$

-   Pierwiastek z błędu średniokwadratowego (ang. *RMSE*)

    $$ \text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left( y_i - \widehat{y_i} \right)^2  } $$

-   Średni błąd absolutny (ang. *MAE*)

    $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^n \left| y_i - \widehat{y_i} \right|$$

```{r}
y = cars$dist
y_hat = model_2$fitted.values
n = nrow(cars)

cor(y, y_hat)^2
sqrt(sum((y - y_hat)^2) / n)
sum(abs(y - y_hat)) / n

caret::R2(y_hat, y)
caret::RMSE(y_hat, y)
caret::MAE(y_hat, y)
```

Jeśli mamy kilka modeli, to przy wyborze ostatecznego modelu mogą pomóc **kryteria informacyjne**. Najpopularniejsze z nich to AIC oraz BIC. Im mniejsza wartość, tym lepszy model.

```{r}
AIC(model_1)
AIC(model_2)

BIC(model_1)
BIC(model_2)
```

### Przykład z wieloma zmiennymi objaśniającymi

Równanie regresji rozszerzamy o dodatkowe zmienne $x$ oraz dodatkowe parametry $\beta$, które musimy oszacować:

$$
y_i = \beta_0 + \beta_1 \ x_{i1} + \dots + \beta_k x_{ik} + \epsilon_i
$$

```{r}
df = read.csv(
  'data\\insurance.csv'
  , stringsAsFactors = TRUE
)
```

```{r}
GGally::ggpairs(df, progress = FALSE)

df %>%
  select_if(is.numeric) %>%
  GGally::ggpairs(progress = FALSE)
```

```{r}
model_1 = lm(charges ~ ., data = df)
summary(model_1)
```

```{r}
model_2 = step(model_1)
summary(model_2)
```

### Zadania

1.  Wczytaj zbiór danych `kc_house_data.csv`. Usuń niepotrzebne zmienne.
2.  Podziel dane na zbiór treningowy, walidacyjny oraz testowy.
3.  Wybierz podzbiór zmiennych objaśniających, aby zminimalizować błąd prognozy na zbiorze walidacyjnym.
4.  Dokonaj finalnej oceny modelu na zbiorze testowym.

## Regresja logistyczna

W tym wypadku zmienna celu jest binarna, tzn. przyjmuje wartości 0 lub 1. Oznaczmy $p_i \in \left[ 0,1 \right]$ jako prawdopodobieństwo, że $y_i =1$. Równanie zwane **modelem logitowym** ma postać::

$$
\ln \left( \frac{p_i}{1-p_i} \right) = \beta_0 + \beta_1  x_{i1} + \dots \beta_k x_{ik} + \epsilon
$$

#### Krok 1: Przygotowanie danych

```{r}
data(Default)
```

```{r}
table(Default$default)
```

Problem niezbalansowania zmiennych - modele predykcyjne mogą być obciążone w kierunku liczniejszej klasy, tzw. *Acurracy paradox*. Kilka metod na obejście tego problemu:

-   *undersampling* - ryzyko utraty informacji z pominiętych obserwacji

-   *oversampling* - ryzyko przetrenowania, utrata niezależności zbioru treningowego oraz zbioru testowego

-   *rose* - nowe obserwacje są generowane z warunkowego KDE obu klas.

```{r}
df = caret::downSample(
  x = Default %>% select(-default)
  , y = Default %>% pull(default)
  , yname = 'default'
)
```

```{r}
GGally::ggpairs(
  df
  , progress = FALSE
  , mapping = aes(fill = default, color = default)
)
```

#### Krok 2: Trenowanie modelu

```{r}
model_log = glm(
  default ~ .
  , data = df
  , family = binomial(link = 'logit')
)
```

#### Krok 3: Ocena modelu

Funkcja `glm` w przypadku regresji logitycznej podaje współczynniki dla *non-reference level* danego faktora, w tym wypadku będzie to odpowiedź *Yes*.

```{r}
summary(model_log)
```

#### Krok 4: Prognozowanie

```{r}
new_data = data.frame(
  student = 'No'
  , balance = 1000
  , income = 24000
)

predict(model_log, newdata = new_data, type = 'response')
```

Zarówno `fitted.values` jak i wynik funkcji `predict` jest wartością z przedziału $[0,1]$, zatem musimy ustalić jakiś próg, aby zaklasyfikować prognozy do odpowiednich kategorii.

```{r}
y = df$default
y_hat_prob = model_log$fitted.values %>% unname()
hist(y_hat_prob)

y_hat = ifelse(y_hat_prob > 0.5, 'Yes', 'No') %>%
  as.factor()

levels(y) == levels(y_hat)
```

**Macierz kontyngencji** podaje nam łączny rozkład zmiennej $y$ oraz prognoz $\widehat{y}$. Poszczególne elementy tej macierzy nazywamy następująco:

-   TP - *true positive* (1,1)

-   FN - *false negative* (1,2)

-   FP - *false positive* (2,1)

-   TN - *true negative* (2,2)

Na podstawie macierzy kontyngencji możemy obliczyć kilka często używanych metryk jakości prognoz:

-   Dokładność (ang. *accuracy*)

    $$\text{ACC} = \frac{\text{TP + TN}}{\text{P + N}}$$

-   Czułość (ang. *sensitivity*, *recall*)

    $$
    \text{TPR} = \frac{\text{TP}}{\text{P}}
    $$

-   Specyfikacja (ang. *specificity*)

    $$
    \text{TNR} = \frac{\text{TN}}{\text{N}}
    $$

-   Precyzja (ang. *precision*)

    $$\text{PPV} = \frac{\text{TP}}{\text{PP}}$$

-   Współczynnik $F_1$

    $$ F_1 = \frac{\text{2TP}}{\text{2TP + FP + FN}} $$

Przydatny link: <https://en.wikipedia.org/wiki/Sensitivity_and_specificity> lub dokumentacja funkcji `caret::confusionMatrix`.

Do porównywania kilku modeli możemy też wykorzystać kryteria informacyjne AIC lub BIC.

```{r}
table(y, y_hat)
```

```{r}
confusionMatrix(
  data = y_hat
  , reference = y
)
```

Jak widzieliśmy powyżej, funkcja `predict` przewiduje prawdopodobieństwo zdarzenia, że użytkownik karty kredytowej zbankrutuje. Jako próg klasyfikacji przyjęliśmy $50\%$, aczkolwiek możemy przyjąć zupełnie inny próg. Zależnie od przedmiotu badań, ten próg może być mniejszy lub większy. Na przykład przy diagnozowaniu pacjentów, jeśli model przewiduje, że prawdopodobieństwo zachorowania pacjenta wynosi $10\%$, to klasyfikujemy go do grupy ryzyka i poddajemy dalszym badaniom.

Jak zatem zmieniają się posczególne metryki błędów w zależności od przyjętego progu?

-   ROC (*receiver operating characteristic*) - na osi $x$ zaznaczamy FPR, na osi $y$ zaznaczamy TPR

-   AUC (*area under curve*) - pole pod wykresem ROC:

    -   wartość $0.5$ miałby model który po prostu by strzelał, tzn. w sposób losowy przydzielał obserwacje do poszczególnych klas

    -   wartość $1$ ma model idealnie odseparowujący obie klasy

    -   wartość $0$ ma model, który przypisuje obserwacje do przeciwnej klasy (błąd w kodowaniu zmiennych?)

Chcemy zatem aby AUC było jak największe i koniecznie powyżej wartości $0.5$.

```{r}
roc = data.frame(
  Threshold = seq(0, 1, by = 0.01)
  , TPR = NA
  , FPR = NA
)

for (i in 1:nrow(roc)) {
  
  y_hat_i = ifelse(y_hat_prob > roc$Threshold[i], 'Yes', 'No') %>%
    as.factor()
  
  conf_matrix = table(y, y_hat_i) %>% as.matrix()
  
  roc$TPR[i] = conf_matrix[1, 1] / sum(conf_matrix[1, ])
  roc$FPR[i] = conf_matrix[2, 1] / sum(conf_matrix[2, ])
  
}

ggplot(roc, aes(x = FPR, y = TPR)) +
  geom_line(size = 1) +
  geom_abline(intercept = 0, slope = 1) +
  ylim(c(0,1))
```

```{r}
pROC::auc(y, y_hat_prob)
pROC::roc(y, y_hat_prob) %>% plot()
```

### Zadania

1.  Wczytaj zbiór danych `heart.disease.csv`
2.  Przygotuj dane: usuń lub zastąp wartości brakujące, sprawdź czy są wartości odstające (tzw. *outliery*), podziel dane zbiór treningowy oraz testowy.
3.  Zbuduj model regresji logistycznej, aby zminimalizować błąd walidacyjny.
4.  Dokonaj finalnej oceny modelu na zbiorze testowym.

Jeśli algorytm błędnie zaklasyfikuje zdrowego pacjenta jako chorego to nic złego się nie stanie - pacjent zostanie poddany dodatkowym badaniom, które wykluczą chorobę. Natomiast gdy algorytm będzie błędnie klasyfikował chorych pacjentów jako zdrowych - to taki model nie będzie się nadawał do praktycznych zastosowań. Częstym wymogiem jest osiągnięcie przez model określonego poziomu specyfikacji (np. TN / N powyżej 95%) zamiast maksymalizacji dokładności.

5.  Co możemy zrobić, aby zwiększyć poziom specyfikacji modelu?

## Algorytm k najbliższych sąsiadów

Bardzo prosty i dosyć intuicyjny algorytm, które możemy wykorzystać zarówno do klasyfikacji jak i do regresji. Zacznijmy od klasyfikacji.

### Klasyfikacja

```{r}
set.seed(213)
train_indices = sample(
  1:nrow(df)
  , size = 100
  , replace = FALSE
)

df_train = df[train_indices, ]
df_test = df[-train_indices, ]

ggplot(
  df_train
  , aes(x = income, y = balance)
) +
  geom_point(aes(color = default))
```

Algorytm działa następująco:

1.  Wybieramy liczbę naturalną $k$.

2.  Dla każdej nowej obserwacji liczymy jej *odległość* do wszystkich obserwacji w zbiorze treningowym.

3.  Sortujemy odległości od najmniejszej do największej.

4.  Nowej obserwacji przypisujemy klasę do której należy większość z jej $k$ sąsiadów. Gdy $k$ jest parzyste to mogą wystąpić tzw. *ties* - wówczas przypisujemy klasę najbliższego sąsiada lub redukujemy $k$ o $1$ lub przypisujemy klasę losowo.

Kluczowym pojęciem jest tutaj odległość. Możemy zastosować różne definicje odległości, natomiast najczęściej stosowaną jest zwykła metryka euklidesowa. Musimy jednak ustandaryzować zmienne, ponieważ większość metryka, a w szczególności metryka euklidesowa, są wrażliwe na skalę.

```{r}
df_train_scaled = df_train %>%
  select(balance, income) %>%
  scale()

df_test_scaled = df_test %>%
  select(balance, income) %>%
  scale()
```

W języku R implementację powyższego algorytmu znajdziemy w funkcji `knn` z biblioteki `class`.

```{r}
y_hat = knn(
  df_train_scaled
  , df_test_scaled
  , cl = df_train$default
  , k = 3
)
```

Teraz możemy porównać z klasami ze zbioru testowego.

```{r}
confusionMatrix(y_hat, df_test$default)
```

Wizualizacja całej przestrzeni podzielonej granicą decyzyjną na dwa obszary.

```{r}
df_boundary = expand.grid(
  x = seq(min(df_train_scaled[, 1]), max(df_train_scaled[, 1]), by = 0.01)
  , y = seq(min(df_train_scaled[, 2]), max(df_train_scaled[, 2]), by = 0.01)
)

df_boundary = df_boundary %>%
  mutate(KNN = knn(
    df_train_scaled
    , test = df_boundary %>% select(x, y)
    , cl = df_train$default
    , k = 3
  ))

ggplot() +
  # Decision boundary
  geom_tile(
    data = df_boundary
    , aes(x = x, y = y, fill = KNN)
    , alpha = 0.3
  ) +
  # Original data
  geom_point(
    data = df_train %>%
      mutate(income = scale(income), balance = scale(balance))
    , aes(x = income, y = balance, color = default)
    , size = 3
  ) +
  theme(legend.position = 'none')
```

### Regresja

Algorytm działa bardzo podobnie jak w przypadku klasyfikacji. Nowej obserwacji przypisujemy średnią wartość zmiennej celu jej $k$ najbliższych sąsiadów. Oczywiście zamiast średniej możemy wybrać inną statystykę, np. medianę.

```{r}
df = read.csv(
  'data\\insurance.csv'
  , stringsAsFactors = TRUE
)

ggplot(df, aes(x = age, y = bmi)) +
  geom_point(aes(color = charges))

set.seed(213)
train_indices = sample(1:nrow(df), 0.5*nrow(df))
df_train_scaled = df[train_indices, ] %>%
  select(bmi, age) %>%
  scale()

df_test_scaled = df[-train_indices, ] %>%
  select(bmi, age) %>%
  scale()

model_knn = knnreg(
  x = df_train_scaled
  , y = df$charges[train_indices]
  , k = 3
)

y_hat = predict(model_knn, newdata = df_test_scaled)

RMSE(y_hat, df$charges[-train_indices])
```

### Zadania

1.  Znajdź optymalną wartość $k$ dla danych `Default`, tak aby zminimalizować błąd walidacyjny.
2.  Znajdź optymalną wartość $k$ dla danych `Insurance`, tak aby zminimalizować błąd walidacyjny.
3.  Napisz funkcję, która podzieli zbiór danych na zbiór treningowy i testowy w zadanych proporcjach. Opcjonalnie funkcja może dokonywać skalowanie zmiennych. Będziemy z tej funkcji korzystać przy poznawaniu kolejnych algorytmów.
