---
title: "Regresja liniowa i regresja logistyczna"
author: "Paweł Wieczyński"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(broom)
library(car)
library(GGally)
library(caret)
library(ISLR)
library(pROC)
options(scipen = 20)
theme_set(theme_bw())
```

## Regresja liniowa

### Przykład z jedną zmienną objaśniającą

#### Krok 1: Przygotowanie danych

-   wczytujemy dane

-   sprawdzamy strukturę danych, podstawowe statystyki opisowe

-   sprawdzamy dane wizualnie za pomocą wykresów

-   modyfikacja zmiennych, dodanie nowych zmiennych

-   podział danych na zbiór uczący, zbiór testowy, zbiór walidacyjny (pominiemy w tym przykładzie).

```{r}
cars = datasets::cars
str(cars)
summary(cars)
```

```{r}
cars$speed = cars$speed * 1.609344
cars$dist = cars$dist * 0.3048
summary(cars)
```

```{r}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point()

cars %>%
  pivot_longer(
    cols = 1:2
    , names_to = 'Variable'
    , values_to = 'Value') %>%
  ggplot(aes(x = Variable, y = Value, fill = Variable)) +
  geom_boxplot() +
  theme(legend.position = 'none')
```

#### Krok 2: Trenowanie modelu

Równanie regresji liniowej ma postać:

$$ y_i = \alpha + \beta \ x_i + \epsilon_i $$

dla $i = 1, 2, \dots, n$ gdzie:

-   $y_i$ nazywamy zmienną zależną / zmienną objaśnianą / zmienną celu

-   $x_i$ nazywamy zmienną niezależną / zmienną objaśniającą / regresorem

-   $\alpha, \beta$ to parametry regresji, które chcemy oszacować

-   $\epsilon_i$ to składnik losowy / reszty modelu, czyli różnice między wartościami obserwacji $y_i$ a wartościami prognozowanymi przez model $\widehat{y_i}$.

Spośród wszystkich możliwych kombinacji $\alpha, \beta$ wybieramy takie $\widehat{\alpha}, \widehat{\beta}$, które minimalizują sumę kwadratów reszt, tzn.

$$ \sum_{i=1}^n \epsilon_i^2 = \sum{i=1}^n \left( y_i - \widehat{y} \right)^2 \ \longrightarrow \ \min $$ Jest to tzw. **metoda najmniejszych kwadratów** (ang. metoda OLS).

```{r}
model_1 = lm(dist ~ speed, data = cars)
model_2 = lm(dist ~ I(speed^2), data = cars)
```

#### Krok 3: Ocena modelu

Za pomocą funkcji *summary()* wyświetlamy podsumowanie modelu. Elementy, na które warto zwrócić uwagę, to:

-   *Estimate* - oszacowania współczynników regresji $\widehat{\alpha}, \widehat{\beta}$

-   *Pr(\>\|t\|)* - wartości *p-value* oznaczające czy dane współczynniki regresji są istotne statystycznie; na ogół przyjmujemy, że współczynniki są istotne, jeżeli *p-value* jest mniejsze od $0.05$

-   *F-statistic* - test sprawdzający, czy model jako całość jest istotny statystycznie; na ogół przyjmujemy, że model jest istotny, jeśli *p-value* odpowiadająca temu testowi jest mniejsza od $0.05$

-   *Multiple R-squared* - współczynnik determinancji $R^2$ oznacza jaki odsetek zmienności zmiennej zależnej został wyjaśniony przez ten model. Wartość $R^2$ przyjmuje wartości od $0$ do $1$, przy czym im większa wartość, tym lepiej dopasowany model.

-   *Adjusted R-squared* - skorygowany $R^2$ nakłada dodatkową penalizację na ilość zmiennych w modelu.

```{r}
summary(model_1)
summary(model_2)
```

```{r}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(formula = 'y ~ x', method = 'lm', se = FALSE) +
  geom_smooth(formula = 'y ~ I(x^2)', method = 'lm', se = FALSE, color = 'red')
```

Aby progznowanie na podstawie tego modelu było wiarygodne, musi być spełnione kilka założeń:

-   istnieje liniowa zależność między zmienną celu a regresorami

-   reszty modelu mają rozkład normalny

-   reszty modelu mają jednakową wariancję (są homoskedastyczne)

-   reszty modelu nie wykazują autokorelacji

-   w zbiorze danych nie ma obserwacji odstających (ang. *outliers*, *influential values*)

-   zmienne objaśniającę nie są idealnie skorelowane między sobą (w przypadku modelu z więcej niż jednym regresorem).

```{r}
# par(mfrow = c(2,2))
plot(model_1)
plot(model_2)
```

```{r}
shapiro.test(model_1$residuals)
shapiro.test(model_2$residuals)
```

```{r}
ncvTest(model_1)
ncvTest(model_2)
```

```{r}
Box.test(model_1$residuals)
Box.test(model_2$residuals)
```

```{r}
cars_model_1 = augment(model_1)
cars_model_2 = augment(model_2)

ggplot(cars_model_1, aes(.resid)) +
  geom_histogram(color = 'white', bins = 10)

ggplot(cars_model_2, aes(.resid)) +
  geom_histogram(color = 'white', bins = 10)
```

#### Krok 4: Prognozowanie

```{r}
new_data = data.frame(speed = c(30, 50, 80))
predict(model_1, newdata = new_data)
predict(model_2, newdata = new_data)
```

Do oceny jakości prognoz mamy wiele różnych miar. Najpopularniejsze z nich to:

-   Współczynnik $R^2$, który możemy policzyć również jako kwadrat korelacji między $y$ oraz $\widehat{y}$

-   Pierwiastek z błędu średniokwadratowego (ang. *RMSE*)

    $$ \text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left( y_i - \widehat{y_i} \right)^2  } $$

-   Średni błąd absolutny (ang. *MAE*)

    $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^n \left| y_i - \widehat{y_i} \right|$$

```{r}
y = cars$dist
y_hat = model_2$fitted.values
n = nrow(cars)

cor(y, y_hat)^2
sqrt(sum((y - y_hat)^2) / n)
sum(abs(y - y_hat)) / n

caret::R2(y_hat, y)
caret::RMSE(y_hat, y)
caret::MAE(y_hat, y)
```

Jeśli mamy kilka modeli, to przy wyborze ostatecznego modelu mogą pomóc **kryteria informacyjne**. Najpopularniejsze z nich to AIC oraz BIC. Im mniejsza wartość, tym lepszy model.

```{r}
AIC(model_1)
AIC(model_2)

BIC(model_1)
BIC(model_2)
```

### Przykład z wieloma zmiennymi objaśniającymi

Równanie regresji rozszerzamy o dodatkowe zmienne $x$ oraz dodatkowe parametry $\beta$, które musimy oszacować:

$$
y_i = \beta_0 + \beta_1 \ x_{i1} + \dots + \beta_k x_{ik} + \epsilon_i
$$

```{r}
df = read.csv(
  'data\\insurance.csv'
  , stringsAsFactors = TRUE
)
```

```{r}
GGally::ggpairs(df, progress = FALSE)

df %>%
  select_if(is.numeric) %>%
  GGally::ggpairs(progress = FALSE)
```

```{r}
model_1 = lm(charges ~ ., data = df)
summary(model_1)
```

```{r}
model_2 = step(model_1)
summary(model_2)
```

### Zadania

1.  Wczytaj zbiór danych `kc_house_data.csv`. Usuń niepotrzebne zmienne.
2.  Podziel dane na zbiór treningowy, walidacyjny oraz testowy.
3.  Wybierz podzbiór zmiennych objaśniających, aby zminimalizować błąd prognozy na zbiorze walidacyjnym.
4.  Dokonaj finalnej oceny modelu na zbiorze testowym.

## Regresja logistyczna

W tym wypadku zmienna celu jest binarna, tzn. przyjmuje wartości 0 lub 1. Oznaczmy $p_i \in \left[ 0,1 \right]$ jako prawdopodobieństwo, że $y_i =1$. Równanie zwane **modelem logitowym** ma postać::

$$
\ln \left( \frac{p_i}{1-p_i} \right) = \beta_0 + \beta_1  x_{i1} + \dots \beta_k x_{ik} + \epsilon
$$

#### Krok 1: Przygotowanie danych

```{r}
data(Default)
```

```{r}
table(Default$default)
```

Problem niezbalansowania zmiennych - modele predykcyjne mogą być obciążone w kierunku liczniejszej klasy, tzw. *Acurracy paradox*. Kilka metod na obejście tego problemu:

-   *undersampling* - ryzyko utraty informacji z pominiętych obserwacji

-   *oversampling* - ryzyko przetrenowania, utrata niezależności zbioru treningowego oraz zbioru testowego

-   *rose* - nowe obserwacje są generowane z warunkowego KDE obu klas.

```{r}
df = caret::downSample(
  x = Default %>% select(-default)
  , y = Default %>% pull(default)
  , yname = 'default'
)
```

```{r}
GGally::ggpairs(
  df
  , progress = FALSE
  , mapping = aes(fill = default, color = default)
)
```

#### Krok 2: Trenowanie modelu

```{r}
model_log = glm(
  default ~ .
  , data = df
  , family = binomial(link = 'logit')
)
```

#### Krok 3: Ocena modelu

Funkcja `glm` w przypadku regresji logitycznej podaje współczynniki dla *non-reference level* danego faktora, w tym wypadku będzie to odpowiedź *Yes*.

```{r}
summary(model_log)
```

#### Krok 4: Prognozowanie

```{r}
new_data = data.frame(
  student = 'No'
  , balance = 1000
  , income = 24000
)

predict(model_log, newdata = new_data, type = 'response')
```

Zarówno `fitted.values` jak i wynik funkcji `predict` jest wartością z przedziału $[0,1]$, zatem musimy ustalić jakiś próg, aby zaklasyfikować prognozy do odpowiednich kategorii.

```{r}
y = df$default
y_hat_prob = model_log$fitted.values %>% unname()
hist(y_hat_prob)

y_hat = ifelse(y_hat_prob > 0.5, 'Yes', 'No') %>%
  as.factor()

levels(y) == levels(y_hat)
```

**Macierz kontyngencji** podaje nam łączny rozkład zmiennej $y$ oraz prognoz $\widehat{y}$. Poszczególne elementy tej macierzy nazywamy następująco:

-   TP - *true positive* (1,1)

-   FN - *false negative* (1,2)

-   FP - *false positive* (2,1)

-   TN - *true negative* (2,2)

Na podstawie macierzy kontyngencji możemy obliczyć kilka często używanych metryk jakości prognoz:

-   Dokładność (ang. *accuracy*)

    $$\text{ACC} = \frac{\text{TP + TN}}{\text{P + N}}$$

-   Czułość (ang. *sensitivity*, *recall*)

    $$
    \text{TPR} = \frac{\text{TP}}{\text{P}}
    $$

-   Specyfikacja (ang. *specificity*)

    $$
    \text{TNR} = \frac{\text{TN}}{\text{N}}
    $$

-   Precyzja (ang. *precision*)

    $$\text{PPV} = \frac{\text{TP}}{\text{PP}}$$

-   Współczynnik $F_1$

    $$ F_1 = \frac{\text{2TP}}{\text{2TP + FP + FN}} $$

Przydatny link: <https://en.wikipedia.org/wiki/Sensitivity_and_specificity> lub dokumentacja funkcji `caret::confusionMatrix`.

Do porównywania kilku modeli możemy też wykorzystać kryteria informacyjne AIC lub BIC.

```{r}
table(y, y_hat)
```

```{r}
confusionMatrix(
  data = y_hat
  , reference = y
)
```

Jak widzieliśmy powyżej, funkcja `predict` przewiduje prawdopodobieństwo zdarzenia, że użytkownik karty kredytowej zbankrutuje. Jako próg klasyfikacji przyjęliśmy $50\%$, aczkolwiek możemy przyjąć zupełnie inny próg. Zależnie od przedmiotu badań, ten próg może być mniejszy lub większy. Na przykład przy diagnozowaniu pacjentów, jeśli model przewiduje, że prawdopodobieństwo zachorowania pacjenta wynosi $10\%$, to klasyfikujemy go do grupy ryzyka i poddajemy dalszym badaniom.

Jak zatem zmieniają się posczególne metryki błędów w zależności od przyjętego progu?

-   ROC (*receiver operating characteristic*) - na osi $x$ zaznaczamy FPR, na osi $y$ zaznaczamy TPR

-   AUC (*area under curve*) - pole pod wykresem ROC:

    -   wartość $0.5$ miałby model który po prostu by strzelał, tzn. w sposób losowy przydzielał obserwacje do poszczególnych klas

    -   wartość $1$ ma model idealnie odseparowujący obie klasy

    -   wartość $0$ ma model, który przypisuje obserwacje do przeciwnej klasy (błąd w kodowaniu zmiennych?)

Chcemy zatem aby AUC było jak największe i koniecznie powyżej wartości $0.5$.

```{r}
roc = data.frame(
  Threshold = seq(0, 1, by = 0.01)
  , TPR = NA
  , FPR = NA
)

for (i in 1:nrow(roc)) {
  
  y_hat_i = ifelse(y_hat_prob > roc$Threshold[i], 'Yes', 'No') %>%
    as.factor()
  
  conf_matrix = table(y, y_hat_i) %>% as.matrix()
  
  roc$TPR[i] = conf_matrix[1, 1] / sum(conf_matrix[1, ])
  roc$FPR[i] = conf_matrix[2, 1] / sum(conf_matrix[2, ])
  
}

ggplot(roc, aes(x = FPR, y = TPR)) +
  geom_line(size = 1) +
  geom_abline(intercept = 0, slope = 1) +
  ylim(c(0,1))
```

```{r}
pROC::auc(y, y_hat_prob)
pROC::roc(y, y_hat_prob) %>% plot()
```
