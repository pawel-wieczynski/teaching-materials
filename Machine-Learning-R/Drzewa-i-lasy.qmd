---
title: "Drzewa decyzyjne i lasy losowe"
author: "Paweł Wieczyński"
format: html
editor: visual
---

```{r}
if(!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse, caret, rpart, rpart.plot, ipred, randomForest)
```

Mamy $k$ zmiennych objaśniających $x_1, \dots, x_n$ oraz zmienną celu $y$ dla $1, \dots, n$ obserwacji.

## Drzewo klasyfikacyjne

Jest to algorytm rekursyjny \[tbd wyjaśnić\]:

1.  W pierwszym kroku wybieramy jedną ze zmiennych objaśniających $x_j$ oraz szukamy wartość tej zmiennej (ozn. ją jako próg $t$), która *najlepiej* klasyfikuje zbiór treningowy na klasy $0$ oraz $1$.
2.  Z kroku pierwszego otrzymaliśmy dwa zbiory tzn. obserwacje dla których $x_j > t$ oraz $x_j \leq t$.
3.  W każdym z tych zbiorów powtarzamy procedurę z kroku pierwszego otrzymując kolejne podzbiory. Całość powtarzamy, aż wszystkie podzbiory końcowe mają mniej niż z góry zadaną ilość obserwacji.

Jak wybrać *najlepszą* klasyfikację zbioru treningowego? tbd zmienić oznaczenie Oznaczmy $\hat{p}$ ilość obserwacji z klasy $1$

tbd wzory na CRE, Gini, Cross entropy

### Krok 1: Przygotowanie danych

```{r}
df = read.csv('data\\wine-quality.csv', stringsAsFactors = TRUE)
map_dbl(df, ~ sum(is.na(.x)))
table(df$quality)
```

```{r}
set.seed(123)
train_index = createDataPartition(df$quality, p = 0.8, list = FALSE)
df_train = df[train_index, ]
df_test = df[-train_index, ]
```

### Krok 2: Trenowanie modelu

W metodzie *rpart*, która jest jedną z implementacji drzew decyzyjnych, mamy kilka hiperparametrów, które możemy kontrolować:

-   *cp (complexity parameter)* - podział drzewa, tzn. jeśli ten nie zmniejszy błędu dopasowania o zadany procent, to podział nie jest dokonywany

-   *maxdepth* - maksymalna głębokość drzewa (początkowy węzeł liczymy jako 0)

-   *minsplit* - minimalna ilość obserwacji jaka musi istnieć w poszczególnych węzłach

-   *minbucket* - minimalna ilość obserwacji jaka musić istnieć w węzłach końcowych

-   *xval* - ilość walidacji krzyżowych.

Ponadto w *rpart* możemy wybrać czy podział ma być wykonany na podstawie indeksu Giniego czy na podstawie TBD

```{r}
model_1 = rpart(
  quality ~ .
  , data = df_train
  , minsplit = 100
  , minbucket = 10
  , cp = 0.01
  , xval = 1
  , parms = list(split = 'gini')
)
# parms = list(split = 'information or gini')
# Duza roznica miedzy information a gini !!!
summary(model_1)

rpart.plot(model_1, type = 4)
```

### Krok 3: Prognozowanie i ocena modelu

```{r}
y = df_test$quality
y_hat = predict(model_1, newdata = df_test) %>%
  .[, 1] %>%
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

## Bagging

Drzewa losowe cechują się dużą wariancją, tzn. niewielka zmiana w danych lub niewielka zmiana parametrów modelu może sprawić, że struktura drzewa będzie wyglądać zupełnie inaczej. Aby ustabilizować wariancję, możemy zastosować procedurę *bootstrap aggregating*:

1.  Losujemy $B$ próbek ze zwracaniem tego samego rozmiaru co dane wejściowe.

2.  Na każdej próbce budujemy drzewo decyzyjne.

3.  Wykonujemy prognozy na zbiorze testowym dla każdego modelu.

4.  Uśredniamy prognozy.

```{r}
model_2 = bagging( 
  quality ~ .
  , data = df_train
  , nbagg = 100
  , coob = TRUE
  
  # Rpart parameters
  , minsplit = 100
  , minbucket = 10
  , cp = 0.01
  , xval = 1
  , parms = list(split = 'gini')
)

# summary(model_2) # don't run
model_2
```

```{r}
y = df_test$quality
y_hat = predict(model_2, newdata = df_test, type = 'prob') %>%
  .[, 1]
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

Przyjrzyjmy się kilku przykładowym drzewom decyzyjnym w procedurze *baggingu*.

```{r}
set.seed(213)
B = 5
n = nrow(df_train)

for (b in 1:B) {
  boot_indices = sample(
    1:n
    , size = n
    , replace = TRUE
  )
  
  df_boot = df_train[boot_indices, ]
  
  boot_model = rpart(
    quality ~ .
    , data = df_boot
    , minsplit = 100
    , maxdepth = 3
    , cp = 0.01
    , xval = 1
    , parms = list(split = 'gini')
  )
  
  rpart.plot(boot_model, type = 4)
}
```

Wszystkie drzewa mają podobną strukturę, tzn. te same zmienne są używane do podziału w każdym kroku. Mówimy, że drzewa są *skorelowane*.

### Lasy losowe

Szczególnym przypadkiem *baggingu* są lasy losowe. Aby uzyskać jeszcze większą losowość, czyli zdekorelować drzewa decyzyjne, możemy w każdym kroku wybierać zmienne objaśniające w sposób losowy.

```{r}
set.seed(213)
B = 5
n = nrow(df_train)
k = ncol(df_train)

for (b in 1:B) {
  boot_indices = sample(
    1:n
    , size = n
    , replace = TRUE
  )
  
  boot_columns = sample(
    1:(k-1)
    , size = 3
    , replace = FALSE
  )
  
  df_boot = df_train[boot_indices, c(boot_columns, k)]
  
  boot_model = rpart(
    quality ~ .
    , data = df_boot
    , minsplit = 100
    , maxdepth = 3
    , cp = 0.01
    , xval = 1
    , parms = list(split = 'gini')
  )
  
  rpart.plot(boot_model, type = 4)
}
```

W bibliotece `randomForest` mamy poprawną oraz efektywną implementację tej procedury. Główne hiperparametry w lasach losowych to:

-    `mtry` - ilość zmiennych objaśniających wybieranych w sposób losowy przy każdym podziale drzewa

-   `ntree` - ilość drzew w lesie :)

```{r}
model_3 = randomForest(
  quality ~.
  , data = df_train
  , mtry = 2
  , ntree = 1000
)

model_3
```

TBD wyjaśnić błąd OOB

```{r}
y = df_test$quality
y_hat = predict(model_3, newdata = df_test, type = 'prob') %>%
  .[, 1]
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

### Optymalizacja hiperparametrów

Biblioteka `caret` zapewnia infrastrukturę to optymalizacji hiperparametrów w wielu algorytmach uczenia maszynowego. Spróbujmy zatem zbudować las losowy, dobierając parametr `mtry`, tak aby uzyskać jak największą wartość *accuracy* na zbiorze testowym.

W funkcji `trainControl` definiujemy metodę (w tym wypadku będzie to walidacja krzyżowa).

```{r}
control = trainControl(
  method = 'cv'
  , number = 5
)
```

W funkcji `train` oprócz formułki i zbioru danych podajemy następujące argumenty:

-   `method` - jaki algorytm uczenia maszynowego chcemy zastosować. List obsługiwanych algorytmów przez bibliotekę `caret`: <https://topepo.github.io/caret/train-models-by-tag.html>

-   `metric` - jaką miarę dopasowania chcemy zoptymalizować

-   `tuneLength` lub `tuneGrid` - siatka hiperparametrów do optymalizacji

-   `trControl` - metoda optymalizacji hiperparametrów

```{r}
model_4 = train(
  quality ~.
  , data = df_train
  , method = 'rf'
  , metric = 'Accuracy'
  , tuneLength = 11
  , trControl = control
)

model_4
```

Wizualizacja dopasowania modelu w zależności od parametru `mtry`:

```{r}
plot(model_4)
```

### Zadania

1.  Zbuduj drzewo regresyjne na zbiorze danych `kc_house_data.csv`.
2.  Zbuduj las losowy na zbiorze danych `kc_house_data.csv`.
3.  Porównaj wyniki z regresją liniową.
